{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40df3864",
   "metadata": {},
   "source": [
    "## Assignment 5 — The Bootstrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ecd774",
   "metadata": {},
   "source": [
    "## Conceptual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca891b49",
   "metadata": {},
   "source": [
    "1. Explain how k-fold cross-validation is implemented.\n",
    "\n",
    "- Randomly split the dataset into **k** roughly equal-sized folds.\n",
    "- For each fold *i*:\n",
    "  - Fit the model on the other **k−1** folds (training set).\n",
    "  - Compute the prediction error on fold *i* (validation set).\n",
    "- Average the **k** validation errors to obtain the k-fold estimate of test error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f090b0",
   "metadata": {},
   "source": [
    "2. What are the advantages and disadvantages of k-fold cross-validation relative to:\n",
    "\n",
    "**i) The validation set approach**\n",
    "\n",
    "- **Advantages:** less sensitive to one random split; uses data more efficiently because every point is used for validation once.\n",
    "- **Disadvantages:** requires fitting the model **k** times, so it is more computationally expensive.\n",
    "\n",
    "**ii) LOOCV**\n",
    "\n",
    "- **Advantages:** usually lower variance in practice and much cheaper when *n* is large (k fits instead of n fits).\n",
    "- **Disadvantages:** slightly higher bias because each model is trained on fewer observations than LOOCV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fddea",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "We will:\n",
    "1. Load the Auto dataset and inspect it\n",
    "2. Estimate standard errors for a **linear regression** model\n",
    "3. Estimate standard errors for a **quadratic regression** model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "# Put Auto.csv in the same folder as this notebook.\n",
    "Auto = pd.read_csv('Auto.csv')\n",
    "\n",
    "# Remove an extra CSV index column if present\n",
    "if 'Unnamed: 0' in Auto.columns:\n",
    "    Auto = Auto.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Ensure horsepower is numeric (some files store missing values as '?')\n",
    "Auto['horsepower'] = pd.to_numeric(Auto['horsepower'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in variables used later\n",
    "Auto = Auto.dropna(subset=['mpg', 'horsepower']).copy()\n",
    "\n",
    "Auto.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f407850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of variables and variable names\n",
    "Auto.shape[1], list(Auto.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bac66",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The output shows the number of variables in the dataset and their names.\n",
    "We will use mpg as the response variable and horsepower as the main predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric variables\n",
    "Auto.drop(columns=['name']).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd78851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations (sample size)\n",
    "Auto.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e63ba",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The summary table describes typical values and ranges of each numeric variable.\n",
    "The printed row count is the sample size used for all model fits and bootstrap resampling below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571838b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (numeric variables only)\n",
    "numeric_data = Auto.drop(columns=['name'])\n",
    "corr_matrix = numeric_data.corr(numeric_only=True)\n",
    "corr_matrix.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6070f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "img = plt.imshow(corr_matrix.values, aspect='auto')\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(corr_matrix.index)), corr_matrix.index)\n",
    "plt.title('Correlation matrix (numeric variables)')\n",
    "plt.colorbar(img)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240da66",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "Key relationships visible in the correlation output:\n",
    "\n",
    "- mpg is strongly negatively correlated with weight, displacement, and horsepower.\n",
    "  This indicates that heavier and more powerful cars tend to have lower fuel efficiency.\n",
    "- cylinders, displacement, horsepower, and weight are strongly positively correlated with each other.\n",
    "  This means these variables carry overlapping information.\n",
    "- year is positively correlated with mpg, suggesting newer model years tend to have higher mpg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ea2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: mpg vs horsepower\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Auto['horsepower'], Auto['mpg'], alpha=0.7)\n",
    "plt.xlabel('horsepower')\n",
    "plt.ylabel('mpg')\n",
    "plt.title('mpg vs horsepower')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f536d",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The scatter plot shows a clear downward trend: as horsepower increases, mpg tends to decrease.\n",
    "The pattern is not perfectly straight, so we will fit both a linear and a quadratic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9550c",
   "metadata": {},
   "source": [
    "### Linear regression model\n",
    "Model: mpg = β0 + β1·horsepower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression model\n",
    "X_lin = sm.add_constant(Auto['horsepower'])\n",
    "y = Auto['mpg']\n",
    "lin_fit = sm.OLS(y, X_lin).fit()\n",
    "\n",
    "lin_fit.params, lin_fit.bse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c875f",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The output lists the estimated intercept and slope, followed by their standard errors from the fitted linear model.\n",
    "A negative slope means higher horsepower is associated with lower mpg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap standard errors for the linear model\n",
    "def bootstrap_linear_se(data, B=1000, seed=1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = data.shape[0]\n",
    "    betas = np.zeros((B, 2))  # intercept, slope\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        sample = data.iloc[idx]\n",
    "        Xb = sm.add_constant(sample['horsepower'])\n",
    "        yb = sample['mpg']\n",
    "        fitb = sm.OLS(yb, Xb).fit()\n",
    "        betas[b] = fitb.params.values\n",
    "    se = betas.std(axis=0, ddof=1)\n",
    "    return se, betas\n",
    "\n",
    "se_boot_lin, betas_lin = bootstrap_linear_se(Auto, B=1000, seed=1)\n",
    "se_boot_lin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e20591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard errors (linear model)\n",
    "linear_compare = pd.DataFrame({\n",
    "    'Estimate': lin_fit.params,\n",
    "    'SE (model)': lin_fit.bse,\n",
    "    'SE (bootstrap)': se_boot_lin\n",
    "})\n",
    "linear_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap distribution of the linear slope\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(betas_lin[:, 1], bins=30)\n",
    "plt.xlabel('Slope estimate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrap distribution of the linear slope')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa845a2",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The comparison table shows two standard error estimates for each coefficient:\n",
    "- **SE (model)**: the usual standard error reported by the fitted regression model\n",
    "- **SE (bootstrap)**: the standard deviation of coefficient estimates across bootstrap resamples\n",
    "\n",
    "If the bootstrap SE is slightly larger, it suggests the resampling-based variability is a bit higher than what the model assumptions imply.\n",
    "The histogram visualizes the variability of the slope estimate across resamples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ce6cc",
   "metadata": {},
   "source": [
    "### Quadratic regression model\n",
    "Model: mpg = β0 + β1·horsepower + β2·horsepower²\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33359590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit quadratic regression model\n",
    "Auto['horsepower2'] = Auto['horsepower'] ** 2\n",
    "X_quad = sm.add_constant(Auto[['horsepower', 'horsepower2']])\n",
    "quad_fit = sm.OLS(y, X_quad).fit()\n",
    "\n",
    "quad_fit.params, quad_fit.bse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf3dcd",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The quadratic model adds a squared horsepower term to allow curvature in the relationship.\n",
    "The output lists coefficient estimates and their standard errors from the fitted quadratic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap standard errors for the quadratic model\n",
    "def bootstrap_quadratic_se(data, B=1000, seed=1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = data.shape[0]\n",
    "    betas = np.zeros((B, 3))  # intercept, hp, hp^2\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        sample = data.iloc[idx].copy()\n",
    "        sample['horsepower2'] = sample['horsepower'] ** 2\n",
    "        Xb = sm.add_constant(sample[['horsepower', 'horsepower2']])\n",
    "        yb = sample['mpg']\n",
    "        fitb = sm.OLS(yb, Xb).fit()\n",
    "        betas[b] = fitb.params.values\n",
    "    se = betas.std(axis=0, ddof=1)\n",
    "    return se, betas\n",
    "\n",
    "se_boot_quad, betas_quad = bootstrap_quadratic_se(Auto, B=1000, seed=1)\n",
    "se_boot_quad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ada26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard errors (quadratic model)\n",
    "quadratic_compare = pd.DataFrame({\n",
    "    'Estimate': quad_fit.params,\n",
    "    'SE (model)': quad_fit.bse,\n",
    "    'SE (bootstrap)': se_boot_quad\n",
    "})\n",
    "quadratic_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter with fitted curves (linear vs quadratic)\n",
    "x = Auto['horsepower'].to_numpy()\n",
    "xg = np.linspace(x.min(), x.max(), 300)\n",
    "\n",
    "yg_lin = lin_fit.predict(sm.add_constant(xg))\n",
    "yg_quad = quad_fit.predict(sm.add_constant(np.column_stack([xg, xg**2])))\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Auto['horsepower'], Auto['mpg'], alpha=0.5)\n",
    "plt.plot(xg, yg_lin, linewidth=2, label='Linear fit')\n",
    "plt.plot(xg, yg_quad, linewidth=2, label='Quadratic fit')\n",
    "plt.xlabel('horsepower')\n",
    "plt.ylabel('mpg')\n",
    "plt.title('mpg vs horsepower with fitted models')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedc340",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "The quadratic model typically follows the curvature in the scatter plot better than the straight-line fit.\n",
    "In many runs, the bootstrap and model-based standard errors are closer for the quadratic model than for the linear model,\n",
    "which is consistent with the quadratic model matching the observed pattern more closely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcf7f7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- We estimated coefficient standard errors for both models using two approaches: model-based SEs and bootstrap SEs.\n",
    "- The linear model captures the overall negative relationship between horsepower and mpg.\n",
    "- The quadratic model allows curvature and often aligns better with the observed trend.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
